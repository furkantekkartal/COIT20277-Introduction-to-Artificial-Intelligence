# W06 - Deep Learning

![picture](./Images/w06_L01.png)

![picture](./Images/w06_L02.png)

![picture](./Images/w06_L03.png)

## Lecture Example 1: Single Neuron Network - W06_Lecture_Page18


In this code, the "neuron" refers to the perceptron, which is a basic unit in artificial neural networks. Let's break down how it's represented:

- Inputs: Imagine you have two switches labeled input1 and input2. These switches can either be on (1) or off (0). They represent the information the neuron receives.
```
for inputs, expected_output in zip(test_cases, expected_outputs):
#inputs;
#Loop1: [0, 0]
#Loop2: [0, 1]
#Loop3: [1, 0]
#Loop4: [1, 1]
```
- Weights: Each switch is connected to the neuron with a wire, and these wires have weights. These weights show how much attention the neuron pays to each switch. In our code, these weights are random numbers, deciding how much each input matters.
```
weights = np.random.rand(2)  # Random weights for two inputs
```
- Bias: Bias is like a natural inclination or tendency for the neuron to lean towards a certain decision regardless of the input. It acts as an extra push. In our code, bias is also a random number.
```
bias = np.random.rand()  # Random bias value
```
- Weighted Sum: The neuron calculates a total based on the inputs and their weights. It's like adding up the values from each switch, but some switches have more weight, so they count more in the total.
```
weighted_sum = np.dot(inputs, weights) + bias
```
- Activation Function: The neuron has a threshold for making decisions. If the total weighted sum is high enough (greater than 1), it decides to output 1, indicating "yes". If it's not high enough (less than or equal to 1), it outputs 0, indicating "no". This is similar to deciding if there's enough energy to turn on a light bulb.
```
def activation_function(x):
    return 1 if x > 1 else 0
```
- Output: The final result, either 1 or 0, shows what the neuron thinks based on the inputs, weights, bias, and activation function. In our case, it's predicting whether the logical AND operation of the inputs should result in 1 or 0.
```
output = activation_function(weighted_sum)
```

So, the neuron is like a decision-making gadget. It takes inputs, considers their importance (weights), adds a bit of bias, makes a decision using an activation function, and gives an output.


### Code of 'Lecture Example 1':

```
### Lecture Example 1: Single Neuron Network - W06_Lecture_Page18

# A perceptron for the logical AND operation of its input
import numpy as np

# Define an activation function called 'activation_function'.
# This function returns 1 if the input 'x' is greater than 1, otherwise it returns 0.
def activation_function(x):
    # Step activation function
    return 1 if x > 1 else 0

# Define a function called 'predict' to make predictions using the perceptron.
# It calculates the weighted sum of inputs, adds bias, and then applies the activation function.
def predict(inputs, weights, bias):
    # Calculate the weighted sum of inputs
    weighted_sum = np.dot(inputs, weights) + bias
    # Apply the activation function to the weighted sum and return the result
    return activation_function(weighted_sum)

# Initialize weights with random values using numpy's random number generator
weights = np.random.rand(2)
# Initialize bias with a random value
bias = np.random.rand()

# Define a function to test the perceptron with logical AND operation
def test_logical_and():
    # Define the truth table for logical AND operation
    test_cases = [[0, 0], [0, 1], [1, 0], [1, 1]]
    # Define the expected outputs for each test case
    expected_outputs = [0, 0, 0, 1]

    # Test each case
    for inputs, expected_output in zip(test_cases, expected_outputs):
        # Predict the output using the perceptron
        output = predict(inputs, weights, bias)
        # Print the inputs, predicted output, and expected output for each test case
        print(f"Inputs: {inputs}, Predicted Output: {output}, Expected Output: {expected_output}")

# Check if this script is being run directly
if __name__ == "__main__":
    # Call the 'test_logical_and' function to run the test cases
    test_logical_and()


'''
Output explanation: 

The issue with the predictions not being correct is likely due to the initial random initialization 
of weights and bias. When you initialize the weights and bias randomly, they are not necessarily set 
to values that enable the perceptron to learn the correct logic for the AND operation immediately.
'''

```

## Lecture Example 2: Multiple Layer Neural Network - W06_Lecture_Page19
```

```

### Tutorial Example 1: Single Neuron Network (logical OR) - W06_Tutorial_Page3
```

```

### Tutorial Example 2: Multiple Layer Neural Network (logical XOR) - W06_Tutorial_Page4
```

```


Output:

![picture](./Images/w05_T01_2.png)






